# Overview

## Description
In this challenge you will evaluate the time-series foundation model [MOIRAI](https://arxiv.org/pdf/2402.02592) on the benchmark [Context Is Key (CiK)](https://arxiv.org/pdf/2410.18959). You will:

1. Clone the CiK repository
2. Implement a MOIRAI predictor class
3. Run the built-in evaluation script
4. Submit your results

## Task

1. **Install the CiK repository**
   - The CiK repository has been cloned into your data folder.
   ```bash
   cd context-is-key-forecasting
   pip install -r requirements.txt
   ```

2. **Implement the MOIRAI predictor**
   - Open `cik_benchmark/baselines/moirai.py`.
   - Complete the `MoiraiForecaster` class so it matches the interface used by other baselines in `cik_benchmark/baselines/`.

3. **Hook MOIRAI into the evaluation script**
   - In `run_baselines.py`, locate and fill in the function `experiment_moirai`.
   - Ensure it instantiates your predictor and returns the `(result_dict, extra_info)` tuple.

4. **Run the evaluation on MOIRAI-small**
   ```python
   from run_baselines import experiment_moirai

   result, _ = experiment_moirai(model_size="small", n_samples=50)
   ```

5. **Save your results**
   ```python
   import pickle
   from pathlib import Path

   submission_folder = Path("<submission_folder_path>")  # e.g. "./submission"

   with open(submission_folder / "submission.pkl", "wb") as f:
       pickle.dump(result, f) #result is your result file
   ```

## Submission Guidelines
- Your only required deliverable is `submission/submission.pkl`.
- Ensure `submission.pkl` was generated by actually running `experiment_moirai()`.
- Your submission should be a dictionary that maps each task name to a list of results from multiple runs. By default, each task is executed five times—-include all five runs in your output.

```python
result = {
    "Task1": [
        {"seed": 1, "score": 0},
        {"seed": 2, "score": 0},
        {"seed": 3, "score": 0},
        {"seed": 4, "score": 0},
        {"seed": 5, "score": 0}
    ],
    # ...
}
```

- You can load the provided `sample_submission.pkl` to inspect the expected structure of the output dictionary—especially useful if you’re writing your own evaluation code instead of using the CiK repository’s built-in scripts.

## Evaluation
- We will compare your `submission.pkl` against our reference output.
- **Score = 100** if your results match exactly.
- **Score = 0** if the file is missing, incorrect, or fabricated without a real run.

> ⚠ Any attempt to “hard-code” or copy the reference results without running the evaluation will be flagged as an integrity violation.
